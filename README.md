# ollama_streamlit

## Features

Interactive UI: Utilize Streamlit to create a user-friendly interface.
Local Model Execution: Run your Ollama models locally without the need for external APIs.
Real-time Responses: Get real-time responses from your models directly in the UI.

## Installation

Before running the app, ensure you have Python installed on your machine. Then, clone this repository and install the required packages using pip:

```bash
git clone https://github.com/zacharias1219/ollama_streamlit.git
```

```bash
cd ollama_streamlit_demos
```

```bash
pip install -r requirements.txt
```

## Usage

To start the app, run the following command in your terminal:

streamlit run 01_üí¨_Chat_Demo.py
Navigate to the URL provided by Streamlit in your browser to interact with the app.

NB: Make sure you have downloaded Ollama to your system.

## Contributing

Interested in contributing to this app?

Great!
I welcome contributions from everyone.
Got questions or suggestions?

Feel free to open an issue or submit a pull request.

## Acknowledgments

üëè Kudos to the Ollama team for their efforts in making open-source models more accessible!
